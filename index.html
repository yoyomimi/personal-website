<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Mingfei Chen</title>
  
  <meta name="author" content="Mingfei Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>
<style>
  .highlightred {
    color: red;
    font-weight: bold;
  }
  </style>
  

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mingfei Chen</name>
              </p>
              <p>I am a third-year Ph.D. student in the Electrical and Computer Engineering department at University of Washington, Seattle. Currently, I am a member of NeuroAI Lab advised by <a href="http://faculty.washington.edu/shlizee/NW/index.html">Prof. Eli Shlizerman</a>.
                My current research interest is multi-modal research for spatial reasoning in 3D scenes, as well as the related applications on multi-modal LLMs, XR devices and robotics. I am grateful to be supported by the <a href="https://research.google/programs-and-events/phd-fellowship/recipients/?filtertab=2025">Google PhD Fellowship</a>.
              </p>
              <p>
                Before that, I recieved my B.S. degree from the Computer Science and Technology department of Huazhong University of Science and Technology of China in 2020. I am also lucky to work with <a href="https://scholar.google.com/citations?user=w2HXPUUAAAAJ&hl=en">Prof. Chang Wen Chen</a> and <a href="https://cse.buffalo.edu/~jsyuan/">Prof. Junsong Yuan</a>,
                  human detection and segmentation in <a href="https://ailab.bytedance.com/">Bytedance AI Lab</a>,
                human-object interaction (HOI) with <a href="http://colalab.org/">Prof. Si Liu</a>,
                multiple-object tracking (MOT) with <a href="https://scholar.google.com/citations?user=4OAHy-kAAAAJ&hl=en">Prof. Jenq-Neng Hwang</a> in <a href="https://ipl-uw.github.io/index.html/">IPL lab</a>, and 3D photo-realistic digital human rendering 
                with <a href="https://yanshuicheng.ai/">Prof. Shuicheng Yan</a> 
                and <a href="https://sites.google.com/site/jshfeng/">Prof. Jiashi Feng</a> in <a href="https://sail.sea.com/">Sea AI Lab</a>,
                and wonderful multi-modality 3D research in Meta Reality Labs.
              </p>
              
              <p style="color:red;">I'm open to research collaboration. Please email me (lasiafly [at] uw.edu) if you are interested to explore more on multi-modal for spatial reasoning in 3D together!</p>

              <p style="text-align:center">
                <a href="mailto:lasiafly@uw.edu">Email</a> &nbsp/&nbsp
                <a href="data/MingfeiCHEN_CV2022.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=uK7MW8QAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/yoyomimi/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/mingfei-chen-b85947153/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://twitter.com/lasiafly">Twitter</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/mfCHEN_profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/mfCHEN_profile-circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
            <tr>
              <td>
                <heading>News</heading>
                <div style="max-height: 400px; overflow-y: auto;">
                  <table width="100%" align="center" border="0" cellpadding="20">
                    <tbody>
                      <tr>
                        <p><li><b>[Oct. 2025]</b> I'm honored to be awarded the <strong><a href="https://research.google/programs-and-events/phd-fellowship/recipients/?filtertab=2025">Google PhD Fellowship 2025</a> in Machine Perception (North America)</strong>!</li></p>
                        <p><li><b>[Sep. 2025]</b> One first-author paper on spatial audio-visual LLMs <strong>got accepted by NeurIPS as Oral (acceptance rate < 0.4%)</strong>!</li></p>
                        <p><li><b>[Jun. 2025]</b> Join Meta Reality Labs again in Redmond, working on reasoning from multi-modal LLMs to human manipulation from egocentric videos!</li></p>
                        <p><li><b>[Mar. 2025]</b> One first-author paper on spatial audio-visual reconstruction <strong>got accepted by CVPR2025 as Highlight (acceptance rate < 2.9%)</strong>!</li></p>
                        <p><li><b>[Sep. 2024]</b> One first-author paper on spatial audio-visual reconstruction <strong>got accepted by NeurIPS2024</strong>!</li></p>
                        <p><li><b>[Jun. 2024]</b> Join Meta Reality Labs in Pittsburgh (now XRCIA Social AI Research group) as a research scientist intern, working with Dr. <a href="https://scholar.google.it/citations?user=5RFgT84AAAAJ&hl=en">Israel D. Gebru</a> and Dr. <a href="https://alexanderrichard.github.io/">Alexander Richard</a>.</li></p>
                        <p><li><b>[May. 2024]</b> Passed my PhD qualify exam!</li></p> 
                        <p><li><b>[Sep. 2023]</b> Present at ICCV2023 <a href="https://av4d.org/">AV4D workshop</a>!</li></p>
                        <p><li><b>[Sep. 2023]</b> Start my Ph.D journey at UW ECE department, NeuroAI Lab!</li></p>
                        <p><li><b>[Jul. 2023]</b> One first-author paper on audio-visual learning <strong>got accepted by ICCV2023</strong>!</li></p>
                        <p><li><b>[Sep. 2022]</b> One co-first author paper on implicit neural acoustic fields <strong>got accepted by NeurIPS2022</strong>!</li></p>
                        <p><li><b>[Jul. 2022]</b> One first-author paper on 3D photo-realistic digital human rendering <strong>got accepted by ECCV2022</strong>!</li></p>
                        <p><li><b>[Jan. 2022]</b> Join NeuroAI Lab, work with <a href="http://faculty.washington.edu/shlizee/NW/index.html">Prof. Eli Shlizerman</a> on audio-visual related research.</li></p> 
                        <p><li><b>[Sep. 2021]</b> Join ECE department at University of Washington, Seattle, as a master student.</li></p> 
                        <p><li><b>[Jun. 2021]</b> Join <a href="https://sail.sea.com/">Sea AI Lab</a> and NUS  <a href="http://lv-nus.org/"> Learning and Vision Lab </a> as research intern, work with <a href="https://yanshuicheng.ai/">Prof. Shuicheng Yan</a> 
                        and <a href="https://sites.google.com/site/jshfeng/">Prof. Jiashi Feng</a> on 3D photo-realistic digital human rendering.</li></p> 
                        <p><li><b>[Mar. 2021]</b> One first-author paper on human-object interaction <strong>got accepted by CVPR2021</strong>!</li></p> 
                        <p><li><b>[Jul. 2020]</b> Join Sensetime Research as research intern, work on human-object interaction.</li></p> 
                        <p><li><b>[Jun. 2020]</b> My thesis on Language-guided Video Retrieval was awarded with <strong>Outstanding undergraduate graduation thesis</strong> of Huazhong University of Science and Technology!</li></p> 
                        <p><li><b>[Sep. 2019]</b> Join <a href="https://ailab.bytedance.com/">Bytedance AI Lab</a> as Computer Vision Algorithm Intern.</li></p> 
                        <p><li><b>[Jul. 2019]</b> Join CUHK (Shenzhen) as research assistant, work with <a href="https://scholar.google.com/citations?user=w2HXPUUAAAAJ&hl=en">Prof. Chang Wen Chen</a> and <a href="https://cse.buffalo.edu/~jsyuan/">Prof. Junsong Yuan</a> on Language-guided Video Retrieval.</li></p>
                      </tr>
                    </tbody>
                  </table>
                </div>
              </td>
            </tr>
          </tbody>
        </table>
      
          
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h3>[1] Multi-Modal LLM for 3D Spatial Reasoning:</h3>
          </td>
        </tr>
      </tbody></table>
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tbody>
          <!-- === SAVVY === -->
          <tr onmouseout="savvy()" onmouseover="savvy()"></tr>
          <tr><td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="savvy_image">
                <img src="images/SAVVY.png" width="160"></div>
              <img src="images/SAVVY.png" width="160">
            </div>
            <script type="text/javascript">
              function savvy() {
                document.getElementById('savvy_image').style.opacity = "1";
              }
              
              function showBibtex() {
                  const bibtexContent = `@misc{chen2025savvy,
                  title={SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing},
                  author={Mingfei Chen and Zijun Cui and Xiulong Liu and Jinlin Xiang and Caleb Zheng and Jingyuan Li and Eli Shlizerman},
                  year={2025},
                  eprint={2506.05414},
                  archivePrefix={arXiv},
                  primaryClass={cs.CV}
              }`;
                
                const modal = document.createElement('div');
                modal.style.cssText = `
                  position: fixed; top: 0; left: 0; width: 100%; height: 100%;
                  background: rgba(0,0,0,0.8); z-index: 1000; display: flex;
                  justify-content: center; align-items: center;
                `;
                
                const content = document.createElement('div');
                content.style.cssText = `
                  background: white; padding: 20px; border-radius: 8px;
                  max-width: 600px; width: 90%; position: relative;
                `;
                
                content.innerHTML = `
                  <h3>BibTeX Citation</h3>
                  <pre style="background: #f5f5f5; padding: 15px; border-radius: 4px; overflow-x: auto;">${bibtexContent}</pre>
                  <button onclick="this.closest('.modal').remove()" style="
                    position: absolute; top: 10px; right: 15px; 
                    background: none; border: none; font-size: 20px; cursor: pointer;
                  ">√ó</button>
                  <button onclick="navigator.clipboard.writeText(\`${bibtexContent}\`).then(() => alert('BibTeX copied to clipboard!'))" 
                    style="margin-top: 10px; padding: 8px 16px; background: #007cba; color: white; border: none; border-radius: 4px; cursor: pointer;">
                    Copy to Clipboard
                  </button>
                `;
                
                modal.className = 'modal';
                modal.appendChild(content);
                document.body.appendChild(modal);
                
                modal.onclick = (e) => {
                  if (e.target === modal) modal.remove();
                };
              }
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing</papertitle>
            <br>
            <strong>Mingfei Chen*</strong>,
            <a href="https://zijuncui.com/">Zijun Cui*</a>,
            <a href="https://dragonliu1995.github.io/">Xiulong Liu*</a>,
            <a href="https://xiangjinlin.com/">Jinlin Xiang</a>,
            <a href="https://www.linkedin.com/in/yang-caleb-zheng/">Caleb Zheng</a>,
            <a href="https://christincha.github.io/">Jingyuan Li</a>,
            <a href="http://faculty.washington.edu/shlizee/NW/index.html">Eli Shlizerman</a>
            <em>Neural Information Processing Systems (NeurIPS)</em>, 2025 <span class="highlightred"><em>(Oral)</em></span>
            <br>
            <a href="https://arxiv.org/pdf/2506.05414">arxiv</a>
            /
            <a href="https://zijuncui02.github.io/SAVVY/">webpage</a>
            /
            <a href="https://zijuncui02.github.io/SAVVY/#demo">demo</a>
            /
            <a href="https://huggingface.co/datasets/ZijunCui/SAVVY-Bench">dataset</a>
            /
            <a href="javascript:void(0)" onclick="showBibtex()">bibtex</a>
            <p>We introduce SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic audio-visual scenes. We also propose SAVVY, a novel training-free pipeline integrating egocentric spatial tracks and dynamic global maps, which significantly enhances AV-LLM performance for improved audio-visual spatial awareness in such environments.</p>
          </td>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h3>[2] Spatial Audio-Visual for 3D Scenes:</h3>
            </td>
          </tr>
        
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <!-- soundvista -->
                  <tr onmouseout="soundvista()" onmouseover="soundvista()"></tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='soundvista_image'>
                        <img src='images/soundvista_intro.png' width="160"></div>
                      <img src='images/soundvista_intro.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function soundvista() {
                        document.getElementById('soundvista_image').style.opacity = "1";
                      }
                    </script>
                  </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>SoundVista: Novel-View Ambient Sound Synthesis via Visual-Acoustic Binding</papertitle>
                      </a>
                      <br>
                      <strong>Mingfei Chen</strong>,
                      <a href="https://scholar.google.it/citations?user=5RFgT84AAAAJ&hl=en">Israel D. Gebru</a>, <a href="https://www.ishwarya.me/">Ishwarya Ananthabhotla</a>, <a href="https://richardt.name/">Christian Richardt</a>, <a href=https://scholar.google.com/citations?user=cyAYD3UAAAAJ&hl=en>Dejan Markovic</a>, <a href="https://www.linkedin.com/in/stevenkrenn">Steven Krenn</a>, 
                      <a href="https://www.linkedin.com/in/todd-keebler-96b94114b/">Todd Keebler</a>, <a href="https://www.linkedin.com/in/jake-sandakly/">Jacob Sandakly</a>, <a href="http://faculty.washington.edu/shlizee/NW/index.html">Eli Shlizerman</a>,
                      <a href="https://alexanderrichard.github.io/">Alexander Richard</a>
                      
                      
                      <br>
                      <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2025 <span class="highlightred"><em>(Highlight)</em></span>
                      <br>
			          <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding_CVPR_2025_paper.pdf">paper</a>
                /
                <a href="https://yoyomimi.github.io/SoundVista.github.io/">webpage</a>
                    /
                            <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34757.png?t=1748546607.7074263">poster</a>
                    /
                          <a href="https://www.youtube.com/watch?v=3iorghOo5yI&t=1s">video</a>
                          /
                          <a href="data/soundvista.bib">bibtex</a>
                      <p>  We introduce SoundVista: a neural network pipeline to generate the ambient sound of arbitrary scene at novel viewpoints, without requiring any constraint or prior knowledge of sound source details. Moreover, our method efficiently adapts to diverse room layouts, reference microphone configurations and unseen environments. </p>
                          
                    </td>
                  </tr>

                                
                  	  <!-- avcloud -->
                      <tr onmouseout="avcloud()" onmouseover="avcloud()"></tr>
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                          <div class="two" id='avcloud_image'>
                            <img src='images/avcloud_intro.png' width="160"></div>
                          <img src='images/avcloud_intro.png' width="160">
                        </div>
                        <script type="text/javascript">
                          function avcloud() {
                            document.getElementById('avcloud_image').style.opacity = "1";
                          }
                        </script>
                      </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>AV-Cloud: Spatial Audio Rendering Through Audio-Visual Cloud Splatting</papertitle>
                          </a>
                          <br>
                          <strong>Mingfei Chen</strong>,
                                            
                          <a href="http://faculty.washington.edu/shlizee/NW/index.html">Eli Shlizerman</a>
                          <br>
                    <em>Neural Information Processing Systems (NeurIPS)</em>, 2024
                          <br>
                          <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/ff1f4141fa2a2d5d5aca6762cfbe6b57-Paper-Conference.pdf">paper</a>
                    /
                           <a href="data/avcloud.bib">bibtex</a>
                           /
                           <a href="https://yoyomimi.github.io/AVSplat.github.io/">webpage</a>
                    /
                            <a href="https://neurips.cc/virtual/2024/poster/92984">poster</a>
                    /
                          <a href="https://yoyomimi.github.io/AVSplat.github.io/static/videos/suppl/AVCloud_video.mp4">video</a>
                        / 
                        <a href="https://github.com/yoyomimi/AV-Cloud/">code</a>
                          <p> AV-Cloud is an audio rendering framework synchronous with the visual perspective. Given video collections, it constructs Audio-Visual Anchors for scene representation and transforms monaural reference sound into spatial audio.</p>
                        </td>
                      </tr>

                       
          
        	  <!-- BEE -->
            <tr onmouseout="bee()" onmouseover="bee()"></tr>
			      <td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one">
                <div class="two" id='bee_image'>
                  <img src='images/bee_intro.png' width="160"></div>
                <img src='images/bee_intro.png' width="160">
              </div>
			        <script type="text/javascript">
			          function bee() {
			            document.getElementById('bee_image').style.opacity = "1";
			          }
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:middle">
			            <papertitle>Be Everywhere - Hear Everything (BEE): Audio Scene Reconstruction by Sparse Audio-Visual Samples</papertitle>
			          </a>
			          <br>
                <strong>Mingfei Chen</strong>,
				  <a href="https://kun-su.netlify.app/">Kun Su</a>,
                                  
			          <a href="http://faculty.washington.edu/shlizee/NW/index.html">Eli Shlizerman</a>
			          <br>
			    <em>International Conference
            on Computer Vision (ICCV)</em>, 2023
			          <br>
			          <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Be_Everywhere_-_Hear_Everything_BEE_Audio_Scene_Reconstruction_by_ICCV_2023_paper.pdf">paper</a>
			    /
                 <a href="data/bee.bib">bibtex</a>
          /
                  <a href="data/Mingfei_04484_BEE_conference_poster.pdf">poster</a>
          /
                <a href="data/BEE-supp-video.mp4">video</a>
			          <p> We introduce a novel method and end-to-end integrated rendering pipeline which allows the listener to be everywhere and hear everything (BEE) in a dynamic scene in real time, based on sparse audio-visual samples.</p>
			        </td>
			      </tr>


	  <!-- INRAS -->
            <tr onmouseout="inras()" onmouseover="inras()"></tr>
			      <td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one">
                <div class="two" id='inras_image'>
                  <img src='images/inras_intro.png' width="160"></div>
                <img src='images/inras_intro.png' width="160">
              </div>
			        <script type="text/javascript">
			          function inras() {
			            document.getElementById('inras_image').style.opacity = "1";
			          }
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:middle">
			            <papertitle>INRAS: Implicit Neural Representation for Audio Scenes</papertitle>
			          </a>
			          <br>
				  <a href="https://kun-su.netlify.app/">Kun Su*</a>,
                                  <strong>Mingfei Chen*</strong>,
			          <a href="http://faculty.washington.edu/shlizee/NW/index.html">Eli Shlizerman</a>
			          <br>
			    <em>Neural Information Processing Systems (NeurIPS)</em>, 2022
			          <br>
			          <a href="https://openreview.net/pdf?id=7KBzV5IL7W">paper</a>
			    /
                 <a href="data/inras.bib">bibtex</a>
          /
                  <a href="data/INRAS_poster.pdf">poster</a>
          /
                <a href="https://recorder-v3.slideslive.com/?share=74099&s=7ff536d7-8327-4ff5-a69f-36ca6f014cdf">video</a>
        /
        <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/35d5ad984cc0ddd84c6f1c177a2066e5-Supplemental-Conference.zip">supplementary with code</a>
        
			          <p> We propose an Implicit Neural Representation for Audio Scenes, INRAS, for efficient representation of spatial audio fields with high fidelity.</p>
			        </td>
			      </tr>
          </tbody></table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h3>[3] 3D Vision:</h3>
                <!-- <p>
                  Representative papers are <span class="highlight">highlighted</span>
                </p> -->
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- gp-nerf -->
            <tr onmouseout="gpnerf()" onmouseover="gpnerf()"></tr>
			      <td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one">
                <div class="two" id='gpnerf_image'>
                  <img src='images/gpnerf_intro.jpg' width="160"></div>
                <img src='images/gpnerf_intro.jpg' width="160">
              </div>
			        <script type="text/javascript">
			          function gpnerf() {
			            document.getElementById('gpnerf_image').style.opacity = "1";
			          }
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:middle">
			            <papertitle>Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering</papertitle>
			          <br>
                <strong>Mingfei Chen</strong>,
			          <a href="http://jeff95.me/">Jianfeng Zhang</a>,
			          <a href="https://sites.google.com/view/xiangyuxu/">Xiangyu Xu</a>, 
			          <a href="https://scholar.google.com/citations?user=nANxp5wAAAAJ&hl=zh-CN/">Lijuan Liu</a>,
			          <a href="https://yujuncai.netlify.app/">Yujun Cai</a>,
			          <a href="https://sites.google.com/site/jshfeng/">Jiashi Feng</a>,
                <a href="https://yanshuicheng.ai/">Shuicheng Yan</a>
			          <br>
			    <em>European Conference on Computer Vision (ECCV)</em>, 2022
			          <br>
			          <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830224.pdf">paper</a>
			    /
                 <a href="data/gpnerf.bib">bibtex</a>
          /
                  <a href="data/5198-poster.pdf">poster</a>
          /
                <a href="data/5198.mp4">video</a>
        /
			          <a href="https://github.com/sail-sg/GP-Nerf">code</a>
			          <p> We develop a geometry-guided generalizable and efficient Neural Radiance Field (NeRF) pipeline for high-fidelity free-viewpoint human body synthesis under settings with sparse camera views.</p>
			        </td>
			      </tr>
        
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h3>[4] Visual Relationship:</h3>
              <!-- <p>
                Representative papers are <span class="highlight">highlighted</span>
              </p> -->
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <!-- hoi -->
            <tr onmouseout="hoi_stop()" onmouseover="hoi_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hoi21_image'>
                  <img src='images/hoi21_intro.jpg' width="160"></div>
                <img src='images/hoi21_intro.jpg' width="160">
              </div>
			        <script type="text/javascript">
			          function hoi_start() {
			            document.getElementById('hoi21_image').style.opacity = "1";
			          }

			          function hoi_stop() {
			            document.getElementById('hoi21_image').style.opacity = "0";
			          }
			          hoi_stop()
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:middle">
			            <papertitle>Reformulating HOI Detection As Adaptive Set Prediction</papertitle>
			          <br>
			          <strong>Mingfei Chen*</strong>,
			          <a href="https://liaoyue.net/">Yue Liao*</a>,
			          <a href="http://colalab.org/">Si Liu</a>, 
			          <a href="https://zyc.ai/">Zhiyuan Chen</a>,
			          <a href="http://wangfei.info/">Fei Wang</a>,
			          <a href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en/">Chen Qian</a>
			          <br>
			    <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2021
			          <br>
			          <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Reformulating_HOI_Detection_As_Adaptive_Set_Prediction_CVPR_2021_paper.pdf">paper</a>
          /
                <a href="data/hoi21.bib">bibtex</a>
          /
                <a href="data/1453_poster.pdf">poster</a>
			    /
			          <a href="https://github.com/yoyomimi/AS-Net">code</a>
			          <p></p>
			          <p> We reformulate HOI detection as an adaptive set prediction problem, with this novel formulation, we propose an Adaptive Set-based one-stage framework (AS-Net) with parallel instances and interaction branches.</p>
			        </td>
			      </tr>

         <!-- trmot -->
                      <tr onmouseout="trmot_stop()" onmouseover="trmot_start()"></tr>
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                          <div class="two" id='trmot_image'>
                            <img src='images/trmot_main.jpg' width="160"></div>
                          <img src='images/trmot_main.jpg' width="160">
                        </div>
                        <script type="text/javascript">
                          function trmot_start() {
                            document.getElementById('trmot_image').style.opacity = "1";
                          }
          
                          function trmot_stop() {
                            document.getElementById('trmot_image').style.opacity = "0";
                          }
                          trmot_stop()
                        </script>
                      </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>TR-MOT: Multi-Object Tracking by Reference</papertitle>
                          <br>
                          <strong>Mingfei Chen</strong>,
                          <a href="https://liaoyue.net/">Yue Liao</a>,
                          <a href="http://colalab.org/">Si Liu</a>, 
                          <a href="http://wangfei.info/">Fei Wang</a>,
                          <a href="https://scholar.google.com/citations?user=4OAHy-kAAAAJ&hl=en">Jenq-Neng Hwang</a>
                          <br>
                    <em>arxiv preprint</em>, 2022
                          <br>
                          <a href="https://arxiv.org/pdf/2203.16621.pdf">arXiv</a>
                    /
                          <a href="data/trmot.bib">bibtex</a>
                          <p></p>
                          <p> We propose a novel Reference Search (RS) module to provide a more reliable association based on the deformable transformer structure, which is natural to learn the feature alignment for each object among frames.</p>
                        </td>
                      </tr>

            </tbody></table>
	
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Graduate Teaching</heading>
              <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                <tr>
                   <p><li><b>[Spring 2022 TA]</b> UW EE 596: Introduction to Deep Learning Applications and Theory (<a href="https://canvas.uw.edu/courses/1547115">website</a>).</li></p>
                   <p><li><b>[Lead TA]</b> UW EE 497/498/596/598: Engineering Entrepreneurial Capstone (<a href="https://www.ece.uw.edu/news-events/capstone-fair/">website</a>).</li></p>
                  </tr>
              </table>
            </td>
          </tr>
        </tbody></table>


				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
              <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                <tr>
                   <p><li><b>Reviewer for</b>: CVPR, NeurIPS, AAAI, ICLR, ICML, IEEE TPAMI.</li></p>
                   <p><li><b>Student organizer for</b>: <a href="https://github.com/shlizee/NeuroAI">University of Washington NeuroAI Seminar</a>.</li></p>
                   <p><li><b>Session chair for</b>: ICCV2023 <a href="https://av4d.org/">AV4D workshop</a>.</li></p>
                </tr>
              </table>
            </td>
          </tr>
        </tbody></table>
	

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards & Honors</heading>
              <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                <tr>
                  <p><li><b>[2025]</b> <a href="https://research.google/programs-and-events/phd-fellowship/recipients/?filtertab=2025">Google PhD Fellowship 2025</a> in Machine Perception (North America).</li></p> 
                  <p><li><b>[2023]</b> International Conference on Computer Vision Diversity, Equity & Inclusion (DEI) Award.</li></p> 
                   <p><li><b>[2020]</b> Huazhong University of Science and Technology Outstanding undergraduate graduation thesis.</li></p> 
                   <p><li><b>[2018, 2019]</b> Huazhong University of Science and Technology Merit Student Scholarship.</li></p> 
                   <p><li><b>[2018]</b> Meritorious Winner of Mathematical Contest In Modeling (MCM/ICM).</li></p> 
                   <p><li><b>[2018]</b> Excellent Team Prize of Central & Southern China District of Future Lab Image Recognition Algorithm Competition.</li></p> 
                   <p><li><b>[2017]</b> Huazhong University of Science and Technology Undergraduate Excellent Student (Top 1% in 35000).</li></p>
                </tr>
              </table>
            </td>
           
          </tr>
        </tbody></table>

          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=bPuWO4R5dsdJonwD57JwjNSGZ2SbsyDPZzZR1QLI52g&cl=ffffff&w=a"></script>
</body>


</html>
