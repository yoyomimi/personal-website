<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Mingfei Chen</title>
  
  <meta name="author" content="Mingfei Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mingfei Chen</name>
              </p>
              <p>I am a Master student in the Electrical and Computer Engineering department at University of Washington, Seattle. Currently, I am a member of NeuroAI Lab advised by <a href="http://faculty.washington.edu/shlizee/">Prof. Eli Shlizerman</a>,
                focusing on the audio-visual related research including visual-guided neural acoustic representation, sound source localization and separation, as well as the related applications. 
              </p>
              <p>
                Before that, I recieved my B.S. degree from the Computer Science and Technology department of Huazhong University of Science and Technology of China in 2020. I am also lucky to work on language-guided video retrieval with <a href="https://scholar.google.com/citations?user=w2HXPUUAAAAJ&hl=en">Prof. Chang Wen Chen</a> and <a href="https://cse.buffalo.edu/~jsyuan/">Prof. Junsong Yuan</a>,
                  human detection and segmentation in <a href="https://ailab.bytedance.com/">Bytedance AI Lab</a>,
                human-object interaction (HOI) with <a href="http://colalab.org/">Prof. Si Liu</a>,
                multiple-object tracking (MOT) with <a href="https://scholar.google.com/citations?user=4OAHy-kAAAAJ&hl=en">Prof. Jenq-Neng Hwang</a> in <a href="https://ipl-uw.github.io/index.html/">IPL lab</a>, and 3D photo-realistic digital human rendering 
                with <a href="https://yanshuicheng.ai/">Prof. Shuicheng Yan</a> 
                and <a href="https://sites.google.com/site/jshfeng/">Prof. Jiashi Feng</a> in <a href="https://sail.sea.com/">Sea AI Lab</a>.
              </p>
              <p>
                <font color="red">I am looking for positions for PhD in 2023 Spring or 2023 Fall.</font>
              </p>
              <p style="text-align:center">
                <a href="mailto:lasiafly@uw.edu">Email</a> &nbsp/&nbsp
                <a href="data/MingfeiCHEN_CV_202207.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=uK7MW8QAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/yoyomimi/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/mingfei-chen-b85947153/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://twitter.com/lasiafly">Twitter</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/mfCHEN_profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/mfCHEN_profile-circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>News</heading>
              <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                <tr>
		   <p><li><b>[Sep. 2022]</b> One co-first author paper on implicit neural acoustic fields <strong>got accepted by NeurIPS2022</strong>!</li></p>
                   <p><li><b>[Jul. 2022]</b> One first-author paper on 3D photo-realistic digital human rendering <strong>got accepted by ECCV2022</strong>!</li></p>
                   <p><li><b>[Jan. 2022]</b> Join NeuroAI Lab, work with <a href="http://faculty.washington.edu/shlizee/">Prof. Eli Shlizerman</a> on audio-visual related research.</li></p> 
                   <p><li><b>[Sep. 2021]</b> Join ECE department at University of Washington, Seattle, as a master student.</li></p> 
                   <p><li><b>[Jun. 2021]</b> Join <a href="https://sail.sea.com/">Sea AI Lab</a> and NUS  <a href="http://lv-nus.org/"> Learning and Vision Lab </a> as research intern, work with <a href="https://yanshuicheng.ai/">Prof. Shuicheng Yan</a> 
                    and <a href="https://sites.google.com/site/jshfeng/">Prof. Jiashi Feng</a> on 3D photo-realistic digital human rendering.</li></p> 
                   <p><li><b>[Mar. 2021]</b> One first-author paper on human-object interaction <strong>got accepted by CVPR2021</strong>!</li></p> 
                   <p><li><b>[Jul. 2020]</b> Join Sensetime Research as research intern, work on human-object interaction.</li></p> 
                   <p><li><b>[Jun. 2020]</b> My thesis on Language-guided Video Retrieval was awarded with <strong>Outstanding undergraduate graduation thesis</strong> of Huazhong University of Science and Technology!</li></p> 
                   <p><li><b>[Sep. 2019]</b> Join <a href="https://ailab.bytedance.com/">Bytedance AI Lab</a> as Computer Vision Algorithm Intern.</li></p> 
                   <p><li><b>[Jul. 2019]</b> Join CUHK (Shenzhen) as research assistant, work with <a href="https://scholar.google.com/citations?user=w2HXPUUAAAAJ&hl=en">Prof. Chang Wen Chen</a> and <a href="https://cse.buffalo.edu/~jsyuan/">Prof. Junsong Yuan</a> on Language-guided Video Retrieval.</li></p>
                </tr>
              </table>
            </td>
           
          </tr>
        </tbody></table>
        
      
          
           

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <!-- <p>
                Representative papers are <span class="highlight">highlighted</span>. -->
              <!-- </p> -->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					
	  <!-- INRAS -->
            <tr onmouseout="inras()" onmouseover="inras()"></tr>
			      <td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one">
                <div class="two" id='inras_image'>
                  <img src='images/inras_intro.png' width="160"></div>
                <img src='images/inras_intro.png' width="160">
              </div>
			        <script type="text/javascript">
			          function inras() {
			            document.getElementById('inras_image').style.opacity = "1";
			          }

			          function inras() {
			            document.getElementById('inras_image').style.opacity = "0";
			          }
			          inras_stop()
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:middle">
			            <papertitle>INRAS: Implicit Neural Representation for Audio Scenes</papertitle>
			          </a>
			          <br>
				  <a href="https://kun-su.netlify.app/">Kun Su*</a>,
                                  <strong>Mingfei Chen*</strong>,
			          <a href="http://faculty.washington.edu/shlizee/">Eli Shlizerman</a>
			          <br>
			    <em>Neural Information Processing Systems (NeurIPS)</em>, 2022
			          <br>
			          [Coming Soon]
			          <p> We propose an Implicit Neural Representation for Audio Scenes, INRAS, for efficient representation of spatial audio fields with high fidelity.</p>
			        </td>
			      </tr>
	      
          <!-- gp-nerf -->
            <tr onmouseout="gpnerf()" onmouseover="gpnerf()"></tr>
			      <td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one">
                <div class="two" id='gpnerf_image'>
                  <img src='images/gpnerf_intro.jpg' width="160"></div>
                <img src='images/gpnerf_intro.jpg' width="160">
              </div>
			        <script type="text/javascript">
			          function gpnerf() {
			            document.getElementById('gpnerf_image').style.opacity = "1";
			          }

			          function gpnerf() {
			            document.getElementById('gpnerf_image').style.opacity = "0";
			          }
			          gpnerf_stop()
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:middle">
			          <a href="https://arxiv.org/pdf/2112.04312.pdf">
			            <papertitle>Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering</papertitle>
			          </a>
			          <br>
                <strong>Mingfei Chen</strong>,
			          <a href="http://jeff95.me/">Jianfeng Zhang</a>,
			          <a href="https://sites.google.com/view/xiangyuxu/">Xiangyu Xu</a>, 
			          <a href="https://scholar.google.com/citations?user=nANxp5wAAAAJ&hl=zh-CN/">Lijuan Liu</a>,
			          <a href="https://yujuncai.netlify.app/">Yujun Cai</a>,
			          <a href="https://sites.google.com/site/jshfeng/">Jiashi Feng</a>,
                <a href="https://yanshuicheng.ai/">Shuicheng Yan</a>
			          <br>
			    <em>European Conference on Computer Vision (ECCV)</em>, 2022
			          <br>
			          <a href="https://arxiv.org/pdf/2112.04312.pdf">arXiv</a>
			    /
                 <a href="data/gpnerf.bib">bibtex</a>
          /
                  <a href="data/5198-poster.pdf">poster</a>
          /
                <a href="data/5198.mp4">video</a>
        /
			          <a href="https://github.com/sail-sg/GP-Nerf">code</a>
			          <p> We develop a geometry-guided generalizable and efficient Neural Radiance Field (NeRF) pipeline for high-fidelity free-viewpoint human body synthesis under settings with sparse camera views.</p>
			        </td>
			      </tr>
          
         <!-- trmot -->
                      <tr onmouseout="trmot_stop()" onmouseover="trmot_start()"></tr>
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                          <div class="two" id='trmot_image'>
                            <img src='images/trmot_main.jpg' width="160"></div>
                          <img src='images/trmot_main.jpg' width="160">
                        </div>
                        <script type="text/javascript">
                          function trmot_start() {
                            document.getElementById('trmot_image').style.opacity = "1";
                          }
          
                          function trmot_stop() {
                            document.getElementById('trmot_image').style.opacity = "0";
                          }
                          trmot_stop()
                        </script>
                      </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">
                          <a href="https://arxiv.org/pdf/2203.16621.pdf">
                            <papertitle>TR-MOT: Multi-Object Tracking by Reference</papertitle>
                          </a>
                          <br>
                          <strong>Mingfei Chen</strong>,
                          <a href="https://liaoyue.net/">Yue Liao</a>,
                          <a href="http://colalab.org/">Si Liu</a>, 
                          <a href="http://wangfei.info/">Fei Wang</a>,
                          <a href="https://scholar.google.com/citations?user=4OAHy-kAAAAJ&hl=en">Jenq-Neng Hwang</a>
                          <br>
                    <em>arxiv preprint</em>, 2022
                          <br>
                          <a href="https://arxiv.org/pdf/2203.16621.pdf">arXiv</a>
                    /
                          <a href="data/trmot.bib">bibtex</a>
                          <p></p>
                          <p> We propose a novel Reference Search (RS) module to provide a more reliable association based on the deformable transformer structure, which is natural to learn the feature alignment for each object among frames.</p>
                        </td>
                      </tr>

            <!-- hoi -->
            <tr onmouseout="hoi_stop()" onmouseover="hoi_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hoi21_image'>
                  <img src='images/hoi21_intro.jpg' width="160"></div>
                <img src='images/hoi21_intro.jpg' width="160">
              </div>
			        <script type="text/javascript">
			          function hoi_start() {
			            document.getElementById('hoi21_image').style.opacity = "1";
			          }

			          function hoi_stop() {
			            document.getElementById('hoi21_image').style.opacity = "0";
			          }
			          hoi_stop()
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:middle">
			          <a href="https://paperswithcode.com/paper/reformulating-hoi-detection-as-adaptive-set/review/">
			            <papertitle>Reformulating HOI Detection As Adaptive Set Prediction</papertitle>
			          </a>
			          <br>
			          <strong>Mingfei Chen*</strong>,
			          <a href="https://liaoyue.net/">Yue Liao*</a>,
			          <a href="http://colalab.org/">Si Liu</a>, 
			          <a href="https://zyc.ai/">Zhiyuan Chen</a>,
			          <a href="http://wangfei.info/">Fei Wang</a>,
			          <a href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en/">Chen Qian</a>
			          <br>
			    <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2021
			          <br>
			          <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Reformulating_HOI_Detection_As_Adaptive_Set_Prediction_CVPR_2021_paper.pdf">paper</a>
          /
                <a href="data/hoi21.bib">bibtex</a>
          /
                <a href="data/1453_poster.pdf">poster</a>
			    /
			          <a href="https://github.com/yoyomimi/AS-Net">code</a>
			          <p></p>
			          <p> We reformulate HOI detection as an adaptive set prediction problem, with this novel formulation, we propose an Adaptive Set-based one-stage framework (AS-Net) with parallel instances and interaction branches.</p>
			        </td>
			      </tr>

	
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Graduate Teaching</heading>
              <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                <tr>
                   <p><li><b>[Spring 2022 TA]</b> UW EE 596: Introduction to Deep Learning Applications and Theory (<a href="https://canvas.uw.edu/courses/1547115">website</a>).</li></p>
                </tr>
              </table>
            </td>
          </tr>
        </tbody></table>


				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
              <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                <tr>
                   <p><li><b>Reviewer for</b>: <a href="https://aaai.org/Conferences/AAAI-23/">AAAI-23</a>.</li></p>
                </tr>
              </table>
            </td>
          </tr>
        </tbody></table>
	

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards & Honors</heading>
              <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                <tr>
                   <p><li><b>[2020]</b> Huazhong University of Science and Technology Outstanding undergraduate graduation thesis.</li></p> 
                   <p><li><b>[2018, 2019]</b> Huazhong University of Science and Technology Merit Student Scholarship.</li></p> 
                   <p><li><b>[2018]</b> Meritorious Winner of Mathematical Contest In Modeling (MCM/ICM).</li></p> 
                   <p><li><b>[2018]</b> Excellent Team Prize of Central & Southern China District of Future Lab Image Recognition Algorithm Competition.</li></p> 
                   <p><li><b>[2017]</b> Huazhong University of Science and Technology Undergraduate Excellent Student (Top 1% in 35000).</li></p>
                </tr>
              </table>
            </td>
           
          </tr>
        </tbody></table>

          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=bPuWO4R5dsdJonwD57JwjNSGZ2SbsyDPZzZR1QLI52g&cl=ffffff&w=a"></script>
</body>


</html>
